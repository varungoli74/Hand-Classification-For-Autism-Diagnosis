{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will be creating the folds for one-point tracking, six-point tracking, and all-points tracking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "armflapping_videos: 100%|██████████| 50/50 [00:17<00:00,  2.86it/s]\n",
      "control_videos: 100%|██████████| 50/50 [00:23<00:00,  2.13it/s]\n",
      "0it [00:00, ?it/s]INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "50it [01:42,  2.05s/it]\n",
      "50it [01:49,  2.20s/it]\n",
      "/var/folders/37/gk9g67jd759dj9nfbp9jjsyw0000gp/T/ipykernel_12388/4098782028.py:127: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
      "/var/folders/37/gk9g67jd759dj9nfbp9jjsyw0000gp/T/ipykernel_12388/4098782028.py:128: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
      "50it [00:00, 12703.85it/s]\n",
      "50it [00:00, 7692.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 90, 126) (50, 90, 126)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# first track all points\n",
    "\n",
    "K = 5\n",
    "\n",
    "import mediapipe as mp \n",
    "from PIL import Image as im \n",
    "import mediapipe as mp\n",
    "\n",
    "def hand_locations(frame, min_detection_confidence = 0.5, min_tracking_confidence = 0.5): \n",
    "    hands = mp.solutions.hands.Hands(min_detection_confidence=min_detection_confidence, min_tracking_confidence=min_tracking_confidence) # MAKE SURE THIS IS ALL GOOD \n",
    "    results = hands.process(frame.astype('uint8'))\n",
    "    X_locations = [0] * 42 # use 0 as default if the class is not there \n",
    "    Y_locations = [0] * 42 \n",
    "    Z_locations = [0] * 42\n",
    "    x = y = z = 0 \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand, hand_landmark in enumerate(results.multi_hand_landmarks):\n",
    "            for i in range(0, 21):\n",
    "                landmark = hand_landmark.landmark[i]\n",
    "                X_locations[x] = landmark.x\n",
    "                Y_locations[y] = landmark.y\n",
    "                Z_locations[z] = landmark.z\n",
    "                x += 1; y += 1; z += 1; \n",
    "    locations = np.concatenate([X_locations, Y_locations, Z_locations])\n",
    "    hands.close()\n",
    "    return locations \n",
    "\n",
    "# time to actually do calibration. \n",
    "\n",
    "SECONDS_TO_DETECT = 2 # in seconds\n",
    "\n",
    "import os, cv2\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "ARMFLAPPING_VIDEOS = []\n",
    "CONTROL_VIDEOS = []\n",
    "ARMFLAPPING_FPS = [] # store the FPS of all armflapping videos \n",
    "CONTROL_FPS = [] # store the FPS of all control videos \n",
    "\n",
    "# load these same file names \n",
    "import pickle\n",
    "with open(\"FILE_NAMES.pkl\", 'rb') as f: \n",
    "    ARMFLAPPING_FILE_NAMES, CONTROL_FILE_NAMES = pickle.load(f)\n",
    "\n",
    "for video_name in tqdm(ARMFLAPPING_FILE_NAMES, desc = \"armflapping_videos\"): \n",
    "    try: \n",
    "        os.mkdir(\"behavior_data/armflapping/\" + video_name[1:])\n",
    "        cap = cv2.VideoCapture('behavior_data/armflapping/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        i = 0\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break  \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image) \n",
    "\n",
    "            cv2.imwrite(\"behavior_data/armflapping/\" + video_name[1:] + \"/\" + str(i + 1) + \".jpg\", image)\n",
    "            i += 1 \n",
    "\n",
    "        ARMFLAPPING_VIDEOS.append(FRAMES)\n",
    "        ARMFLAPPING_FPS.append(frame_rate)\n",
    "\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(f\"failed on {video_name}\")\n",
    "\n",
    "for video_name in tqdm(CONTROL_FILE_NAMES, desc = \"control_videos\"): \n",
    "    try:\n",
    "        os.mkdir(\"behavior_data/control/\" + video_name[1:])\n",
    "        cap = cv2.VideoCapture('behavior_data/control/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "        i = 0\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image)\n",
    "\n",
    "            cv2.imwrite(\"behavior_data/control/\" + video_name[1:] + \"/\" + str(i + 1) + \".jpg\", image)\n",
    "            \n",
    "            i += 1\n",
    "        CONTROL_VIDEOS.append(FRAMES)\n",
    "        CONTROL_FPS.append(frame_rate)\n",
    "        \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(f\"failed on {video_name}\")\n",
    "\n",
    "ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS = [], []\n",
    "\n",
    "for FRAMES, file_name in tqdm(zip(CONTROL_VIDEOS, CONTROL_FILE_NAMES)):  \n",
    "    locs = []\n",
    "    for i, frame in enumerate(FRAMES): \n",
    "        locs.append(hand_locations(np.array(frame)))\n",
    "\n",
    "        #FRAME = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        #cv2.imwrite(\"behavior_data/control/\" + file_name[1:] + \"/\" + str(i + 1) + \".jpg\", frame)\n",
    "    CONTROL_LOCATIONS.append(locs)\n",
    "\n",
    "\n",
    "# get the locations of all of the videos \n",
    "\n",
    "for FRAMES, file_name in tqdm(zip(ARMFLAPPING_VIDEOS,ARMFLAPPING_FILE_NAMES)) :\n",
    "    locs = []\n",
    "    for i, frame in enumerate(FRAMES): \n",
    "        locs.append(hand_locations(np.array(frame)))\n",
    "\n",
    "        #FRAME = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        #cv2.imwrite(\"behavior_data/armflapping/\" + file_name[1:] + \"/\" + str(i + 1) + \".jpg\", frame)\n",
    "    ARMFLAPPING_LOCATIONS.append(locs)\n",
    "\n",
    "N = min([len(locs) for locs in [ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS]])\n",
    "ARMFLAPPING_LOCATIONS = ARMFLAPPING_LOCATIONS[:N]\n",
    "CONTROL_LOCATIONS = CONTROL_LOCATIONS[:N]\n",
    "ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
    "CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
    "\n",
    "ARMFLAPPING_FILE_NAMES = np.array(ARMFLAPPING_FILE_NAMES[:N])\n",
    "CONTROL_FILE_NAMES = np.array(CONTROL_FILE_NAMES[:N])\n",
    "\n",
    "# we can create a padding function in order to pad \n",
    "def pad(locations, maxlen = 90, padding = \"post\", truncating = \"post\"): \n",
    "    new_locations = locations.tolist() \n",
    "    empty_row = np.zeros((1, 126))\n",
    "    for i, video in tqdm(enumerate(new_locations)): \n",
    "        if len(video) < maxlen:  \n",
    "            for new_row in range(maxlen - len(video)): \n",
    "                if padding == \"post\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([new_locations[i], empty_row])\n",
    "                if padding == \"pre\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([empty_row, new_locations[i]])\n",
    "\n",
    "        if len(video) > maxlen: \n",
    "            if truncating == \"post\": \n",
    "                new_locations[i] = new_locations[i][:maxlen]\n",
    "            elif truncating == \"pre\": \n",
    "                new_locations[i] = new_locations[i][len(video) - maxlen : ]\n",
    "    return np.array(new_locations)\n",
    "\n",
    "padded_armflapping_locations = ARMFLAPPING_LOCATIONS\n",
    "padded_control_locations = CONTROL_LOCATIONS\n",
    "padded_armflapping_locations = pad(padded_armflapping_locations, maxlen = 90)\n",
    "padded_control_locations = pad(padded_control_locations, maxlen = 90)\n",
    "print(padded_control_locations.shape, padded_armflapping_locations.shape)\n",
    "assert padded_armflapping_locations.shape == padded_control_locations.shape \n",
    "\n",
    "ARMFLAPPING_FILE_NAMES = np.array(ARMFLAPPING_FILE_NAMES)\n",
    "CONTROL_FILE_NAMES = np.array(CONTROL_FILE_NAMES)\n",
    "\n",
    "\"\"\"\n",
    "# first shuffle \n",
    "N = padded_armflapping_locations.shape[0]\n",
    "perm = np.random.permutation(N)\n",
    "padded_armflapping_locations = padded_armflapping_locations[perm]\n",
    "padded_control_locations = padded_control_locations[perm]\n",
    "ARMFLAPPING_FILE_NAMES = ARMFLAPPING_FILE_NAMES[perm]\n",
    "CONTROL_FILE_NAMES = CONTROL_FILE_NAMES[perm]\n",
    "\"\"\"\n",
    "\n",
    "# then balance each fold (still should do this) \n",
    "padded_armflapping_locations = np.array_split(padded_armflapping_locations, K)\n",
    "padded_control_locations = np.array_split(padded_control_locations, K)\n",
    "ARMFLAPPING_FILE_NAMES = np.array_split(ARMFLAPPING_FILE_NAMES, K)\n",
    "CONTROL_FILE_NAMES = np.array_split(CONTROL_FILE_NAMES, K)\n",
    "\n",
    "X_splits = []\n",
    "y_splits = []\n",
    "\n",
    "for i in range(K): \n",
    "    X_splits.append(np.concatenate([padded_armflapping_locations[i], padded_control_locations[i]]))\n",
    "    y_splits.append(np.concatenate([np.ones((len(padded_armflapping_locations[i]), 1)), np.zeros((len(padded_control_locations[i]), 1))]))\n",
    "\n",
    "import pickle\n",
    "\n",
    "for i, (X_split, y_split, A_FILE_NAMES, C_FILE_NAMES) in enumerate(zip(X_splits, y_splits, ARMFLAPPING_FILE_NAMES, CONTROL_FILE_NAMES)): \n",
    "    with open(f\"all_points_folds/split{i+1}\", 'wb') as f: \n",
    "        pickle.dump((X_split, y_split), f)\n",
    "    \n",
    "\n",
    "with open(f\"all_point_folds_seeds\", \"wb\") as f: \n",
    "    pickle.dump((padded_armflapping_locations, ARMFLAPPING_FILE_NAMES, padded_control_locations, CONTROL_FILE_NAMES), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "armflapping_videos:  12%|█▏        | 13/109 [00:03<00:11,  8.42it/s]OpenCV: Couldn't read video stream from file \"behavior_data/armflapping/.DS_Store\"\n",
      "[ERROR:0] global /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-xxsyexfp/opencv/modules/videoio/src/cap.cpp (162) open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.5.3) /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-xxsyexfp/opencv/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): behavior_data/armflapping/.DS_Store in function 'icvExtractPattern'\n",
      "\n",
      "\n",
      "armflapping_videos:  14%|█▍        | 15/109 [00:03<00:12,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float division by zero\n",
      "failed on .DS_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "armflapping_videos:  54%|█████▍    | 59/109 [00:17<00:06,  7.32it/s]OpenCV: Couldn't read video stream from file \"behavior_data/armflapping/.mp4\"\n",
      "armflapping_videos:  56%|█████▌    | 61/109 [00:17<00:05,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float division by zero\n",
      "failed on .mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "armflapping_videos: 100%|██████████| 109/109 [00:32<00:00,  3.31it/s]\n",
      "control_videos:  19%|█▉        | 12/62 [00:05<00:24,  2.02it/s]OpenCV: Couldn't read video stream from file \"behavior_data/control/.DS_Store\"\n",
      "[ERROR:0] global /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-xxsyexfp/opencv/modules/videoio/src/cap.cpp (162) open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.5.3) /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/pip-req-build-xxsyexfp/opencv/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): behavior_data/control/.DS_Store in function 'icvExtractPattern'\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float division by zero\n",
      "failed on .DS_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "control_videos: 100%|██████████| 62/62 [00:22<00:00,  2.79it/s]\n",
      "50it [01:42,  2.05s/it]\n",
      "97it [03:24,  2.11s/it]\n",
      "/var/folders/37/gk9g67jd759dj9nfbp9jjsyw0000gp/T/ipykernel_4906/1597820171.py:134: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
      "/var/folders/37/gk9g67jd759dj9nfbp9jjsyw0000gp/T/ipykernel_4906/1597820171.py:135: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
      "50it [00:00, 16350.79it/s]\n",
      "50it [00:00, 11582.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 90, 36) (50, 90, 36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# next, track six points\n",
    "\n",
    "text here to make sure you don't run this cell again. just look in six_point_folds_seeds for all data required.  \n",
    "\n",
    "K = 5\n",
    "\n",
    "import mediapipe as mp \n",
    "from PIL import Image as im \n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42) # set a random seed \n",
    "\n",
    "def hand_locations(frame, min_detection_confidence = 0.5, min_tracking_confidence = 0.5): \n",
    "    \"\"\"Only give 6 landmarks\"\"\"\n",
    "\n",
    "    hands = mp.solutions.hands.Hands(min_detection_confidence=min_detection_confidence, min_tracking_confidence=min_tracking_confidence) # MAKE SURE THIS IS ALL GOOD \n",
    "    results = hands.process(frame.astype('uint8'))\n",
    "    X_locations = [0] * 12\n",
    "    Y_locations = [0] * 12\n",
    "    Z_locations = [0] * 12\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        x = y = z = 0 \n",
    "        for hand, hand_landmark in enumerate(results.multi_hand_landmarks):\n",
    "            for i in range(0, 21):\n",
    "                if i not in [0, 4, 8, 12, 16, 20]: continue \n",
    "                landmark = hand_landmark.landmark[i]\n",
    "                X_locations[x] = landmark.x\n",
    "                Y_locations[y] = landmark.y \n",
    "                Z_locations[z] = landmark.z\n",
    "                x += 1; y += 1; z +=1; \n",
    "            \n",
    "    hands.close()\n",
    "    return np.concatenate([X_locations, Y_locations, Z_locations]) \n",
    "\n",
    "# time to actually do calibration. \n",
    "\n",
    "SECONDS_TO_DETECT = 2 # in seconds\n",
    "\n",
    "import os, cv2\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "ARMFLAPPING_VIDEOS = []\n",
    "CONTROL_VIDEOS = []\n",
    "ARMFLAPPING_FPS = [] # store the FPS of all armflapping videos \n",
    "CONTROL_FPS = [] # store the FPS of all control videos \n",
    "\n",
    "import pickle\n",
    "with open(\"FILE_NAMES.pkl\", 'rb') as f: \n",
    "    ARMFLAPPING_FILE_NAMES, CONTROL_FILE_NAMES = pickle.load(f)\n",
    "for video_name in tqdm(ARMFLAPPING_FILE_NAMES, desc = \"armflapping_videos\"): \n",
    "    try: \n",
    "        os.mkdir(\"behavior_data/armflapping/\" + video_name[1:])\n",
    "        cap = cv2.VideoCapture('behavior_data/armflapping/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        i = 0\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break  \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image) \n",
    "\n",
    "            cv2.imwrite(\"behavior_data/armflapping/\" + video_name[1:] + \"/\" + str(i + 1) + \".jpg\", image)\n",
    "            i += 1 \n",
    "\n",
    "        ARMFLAPPING_VIDEOS.append(FRAMES)\n",
    "        ARMFLAPPING_FPS.append(frame_rate)\n",
    "\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(f\"failed on {video_name}\")\n",
    "\n",
    "for video_name in tqdm(CONTROL_FILE_NAMES, desc = \"control_videos\"): \n",
    "    try:\n",
    "        os.mkdir(\"behavior_data/control/\" + video_name[1:])\n",
    "        cap = cv2.VideoCapture('behavior_data/control/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "        i = 0\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image)\n",
    "\n",
    "            cv2.imwrite(\"behavior_data/control/\" + video_name[1:] + \"/\" + str(i + 1) + \".jpg\", image)\n",
    "            \n",
    "            i += 1\n",
    "        CONTROL_VIDEOS.append(FRAMES)\n",
    "        CONTROL_FPS.append(frame_rate)\n",
    "        \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(f\"failed on {video_name}\")\n",
    "\n",
    "ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS = [], []\n",
    "\n",
    "for FRAMES, file_name in tqdm(zip(CONTROL_VIDEOS, CONTROL_FILE_NAMES)):  \n",
    "    locs = []\n",
    "    for i, frame in enumerate(FRAMES): \n",
    "        locs.append(hand_locations(np.array(frame)))\n",
    "\n",
    "        #FRAME = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        #cv2.imwrite(\"behavior_data/control/\" + file_name[1:] + \"/\" + str(i + 1) + \".jpg\", frame)\n",
    "    CONTROL_LOCATIONS.append(locs)\n",
    "\n",
    "\n",
    "# get the locations of all of the videos \n",
    "\n",
    "for FRAMES, file_name in tqdm(zip(ARMFLAPPING_VIDEOS,ARMFLAPPING_FILE_NAMES)) :\n",
    "    locs = []\n",
    "    for i, frame in enumerate(FRAMES): \n",
    "        locs.append(hand_locations(np.array(frame)))\n",
    "\n",
    "        #FRAME = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        #cv2.imwrite(\"behavior_data/armflapping/\" + file_name[1:] + \"/\" + str(i + 1) + \".jpg\", frame)\n",
    "    ARMFLAPPING_LOCATIONS.append(locs)\n",
    "\n",
    "N = min([len(locs) for locs in [ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS]])\n",
    "ARMFLAPPING_LOCATIONS = ARMFLAPPING_LOCATIONS[:N]\n",
    "CONTROL_LOCATIONS = CONTROL_LOCATIONS[:N]\n",
    "ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
    "CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
    "\n",
    "ARMFLAPPING_FILE_NAMES = np.array(ARMFLAPPING_FILE_NAMES[:N])\n",
    "CONTROL_FILE_NAMES = np.array(CONTROL_FILE_NAMES[:N])\n",
    "\n",
    "# we can create a padding function in order to pad \n",
    "def pad(locations, maxlen = 90, padding = \"post\", truncating = \"post\"): \n",
    "    new_locations = locations.tolist() \n",
    "    empty_row = np.zeros((1, 36))\n",
    "    for i, video in tqdm(enumerate(new_locations)): \n",
    "        if len(video) < maxlen:  \n",
    "            for new_row in range(maxlen - len(video)): \n",
    "                if padding == \"post\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([new_locations[i], empty_row])\n",
    "                if padding == \"pre\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([empty_row, new_locations[i]])\n",
    "\n",
    "        if len(video) > maxlen: \n",
    "            if truncating == \"post\": \n",
    "                new_locations[i] = new_locations[i][:maxlen]\n",
    "            elif truncating == \"pre\": \n",
    "                new_locations[i] = new_locations[i][len(video) - maxlen : ]\n",
    "    return np.array(new_locations)\n",
    "\n",
    "padded_armflapping_locations = ARMFLAPPING_LOCATIONS\n",
    "padded_control_locations = CONTROL_LOCATIONS\n",
    "padded_armflapping_locations = pad(padded_armflapping_locations, maxlen = 90)\n",
    "padded_control_locations = pad(padded_control_locations, maxlen = 90)\n",
    "print(padded_control_locations.shape, padded_armflapping_locations.shape)\n",
    "assert padded_armflapping_locations.shape == padded_control_locations.shape \n",
    "\n",
    "ARMFLAPPING_FILE_NAMES = np.array(ARMFLAPPING_FILE_NAMES)\n",
    "CONTROL_FILE_NAMES = np.array(CONTROL_FILE_NAMES)\n",
    "\n",
    "# first shuffle \n",
    "N = padded_armflapping_locations.shape[0]\n",
    "perm = np.random.permutation(N)\n",
    "padded_armflapping_locations = padded_armflapping_locations[perm]\n",
    "padded_control_locations = padded_control_locations[perm]\n",
    "ARMFLAPPING_FILE_NAMES = ARMFLAPPING_FILE_NAMES[perm]\n",
    "CONTROL_FILE_NAMES = CONTROL_FILE_NAMES[perm]\n",
    "\n",
    "# then balance each fold \n",
    "padded_armflapping_locations = np.array_split(padded_armflapping_locations, K)\n",
    "padded_control_locations = np.array_split(padded_control_locations, K)\n",
    "ARMFLAPPING_FILE_NAMES = np.array_split(ARMFLAPPING_FILE_NAMES, K)\n",
    "CONTROL_FILE_NAMES = np.array_split(CONTROL_FILE_NAMES, K)\n",
    "\n",
    "X_splits = []\n",
    "y_splits = []\n",
    "\n",
    "for i in range(K): \n",
    "    X_splits.append(np.concatenate([padded_armflapping_locations[i], padded_control_locations[i]]))\n",
    "    y_splits.append(np.concatenate([np.ones((len(padded_armflapping_locations[i]), 1)), np.zeros((len(padded_control_locations[i]), 1))]))\n",
    "\n",
    "import pickle\n",
    "\n",
    "for i, (X_split, y_split, A_FILE_NAMES, C_FILE_NAMES) in enumerate(zip(X_splits, y_splits, ARMFLAPPING_FILE_NAMES, CONTROL_FILE_NAMES)): \n",
    "    with open(f\"six_point_folds/split{i+1}\", 'wb') as f: \n",
    "        pickle.dump((X_split, y_split), f)\n",
    "    \n",
    "\n",
    "with open(f\"six_point_folds_seeds\", \"wb\") as f: \n",
    "    pickle.dump((padded_armflapping_locations, ARMFLAPPING_FILE_NAMES, padded_control_locations, CONTROL_FILE_NAMES), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "armflapping_videos: 100%|██████████| 50/50 [00:17<00:00,  2.86it/s]\n",
      "control_videos: 100%|██████████| 50/50 [00:24<00:00,  2.07it/s]\n",
      "50it [01:44,  2.09s/it]\n",
      "50it [01:52,  2.25s/it]\n",
      "/var/folders/37/gk9g67jd759dj9nfbp9jjsyw0000gp/T/ipykernel_12388/4245297425.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
      "/var/folders/37/gk9g67jd759dj9nfbp9jjsyw0000gp/T/ipykernel_12388/4245297425.py:130: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
      "50it [00:00, 12713.86it/s]\n",
      "50it [00:00, 16041.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 90, 6) (50, 90, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# finally, only track one point\n",
    "\n",
    "K = 5\n",
    "\n",
    "import mediapipe as mp \n",
    "from PIL import Image as im \n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "def hand_locations(frame, min_detection_confidence = 0.5, min_tracking_confidence = 0.5): \n",
    "    \"\"\"Only give the 0th landmark\"\"\"\n",
    "\n",
    "    hands = mp.solutions.hands.Hands(min_detection_confidence=min_detection_confidence, min_tracking_confidence=min_tracking_confidence)  \n",
    "    results = hands.process(frame.astype('uint8'))\n",
    "    X_locations = [0] * 2\n",
    "    Y_locations = [0] * 2\n",
    "    Z_locations = [0] * 2\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand, hand_landmark in enumerate(results.multi_hand_landmarks):\n",
    "            for i in range(0, 21):\n",
    "                landmark = hand_landmark.landmark[i]\n",
    "                X_locations[hand] = landmark.x\n",
    "                Y_locations[hand] = landmark.y\n",
    "                Z_locations[hand] = landmark.z\n",
    "                break # take only the first landmark\n",
    "            \n",
    "    hands.close()\n",
    "    return np.concatenate([X_locations, Y_locations, Z_locations]) \n",
    "\n",
    "# time to actually do calibration. \n",
    "\n",
    "SECONDS_TO_DETECT = 2 # in seconds\n",
    "\n",
    "import os, cv2\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "ARMFLAPPING_VIDEOS = []\n",
    "CONTROL_VIDEOS = []\n",
    "ARMFLAPPING_FPS = [] # store the FPS of all armflapping videos \n",
    "CONTROL_FPS = [] # store the FPS of all control videos \n",
    "\n",
    "# load these same file names \n",
    "import pickle\n",
    "with open(\"FILE_NAMES.pkl\", 'rb') as f: \n",
    "    ARMFLAPPING_FILE_NAMES, CONTROL_FILE_NAMES = pickle.load(f)\n",
    "\n",
    "for video_name in tqdm(ARMFLAPPING_FILE_NAMES, desc = \"armflapping_videos\"): \n",
    "    try: \n",
    "        os.mkdir(\"behavior_data/armflapping/\" + video_name[1:])\n",
    "        cap = cv2.VideoCapture('behavior_data/armflapping/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "\n",
    "        i = 0\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break  \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image) \n",
    "\n",
    "            cv2.imwrite(\"behavior_data/armflapping/\" + video_name[1:] + \"/\" + str(i + 1) + \".jpg\", image)\n",
    "            i += 1 \n",
    "\n",
    "        ARMFLAPPING_VIDEOS.append(FRAMES)\n",
    "        ARMFLAPPING_FPS.append(frame_rate)\n",
    "\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(f\"failed on {video_name}\")\n",
    "\n",
    "for video_name in tqdm(CONTROL_FILE_NAMES, desc = \"control_videos\"): \n",
    "    try:\n",
    "        os.mkdir(\"behavior_data/control/\" + video_name[1:])\n",
    "        cap = cv2.VideoCapture('behavior_data/control/' + video_name)  \n",
    "        frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if cap.get(cv2.CAP_PROP_FRAME_COUNT) / frame_rate < SECONDS_TO_DETECT: continue # too short! \n",
    "\n",
    "        FRAMES = [] # frames for this video \n",
    "        i = 0\n",
    "        while cap.isOpened(): \n",
    "            _, image = cap.read() \n",
    "            if not _ : \n",
    "                break \n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert to RGB. \n",
    "            FRAMES.append(image)\n",
    "\n",
    "            cv2.imwrite(\"behavior_data/control/\" + video_name[1:] + \"/\" + str(i + 1) + \".jpg\", image)\n",
    "            \n",
    "            i += 1\n",
    "        CONTROL_VIDEOS.append(FRAMES)\n",
    "        CONTROL_FPS.append(frame_rate)\n",
    "        \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(f\"failed on {video_name}\")\n",
    "\n",
    "ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS = [], []\n",
    "\n",
    "for FRAMES, file_name in tqdm(zip(CONTROL_VIDEOS, CONTROL_FILE_NAMES)):  \n",
    "    locs = []\n",
    "    for i, frame in enumerate(FRAMES): \n",
    "        locs.append(hand_locations(np.array(frame)))\n",
    "\n",
    "        #FRAME = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        #cv2.imwrite(\"behavior_data/control/\" + file_name[1:] + \"/\" + str(i + 1) + \".jpg\", frame)\n",
    "    CONTROL_LOCATIONS.append(locs)\n",
    "\n",
    "\n",
    "# get the locations of all of the videos \n",
    "\n",
    "for FRAMES, file_name in tqdm(zip(ARMFLAPPING_VIDEOS,ARMFLAPPING_FILE_NAMES)) :\n",
    "    locs = []\n",
    "    for i, frame in enumerate(FRAMES): \n",
    "        locs.append(hand_locations(np.array(frame)))\n",
    "\n",
    "        #FRAME = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        #cv2.imwrite(\"behavior_data/armflapping/\" + file_name[1:] + \"/\" + str(i + 1) + \".jpg\", frame)\n",
    "    ARMFLAPPING_LOCATIONS.append(locs)\n",
    "\n",
    "N = min([len(locs) for locs in [ARMFLAPPING_LOCATIONS, CONTROL_LOCATIONS]])\n",
    "ARMFLAPPING_LOCATIONS = ARMFLAPPING_LOCATIONS[:N]\n",
    "CONTROL_LOCATIONS = CONTROL_LOCATIONS[:N]\n",
    "ARMFLAPPING_LOCATIONS = np.array(ARMFLAPPING_LOCATIONS)\n",
    "CONTROL_LOCATIONS = np.array(CONTROL_LOCATIONS)\n",
    "\n",
    "ARMFLAPPING_FILE_NAMES = np.array(ARMFLAPPING_FILE_NAMES[:N])\n",
    "CONTROL_FILE_NAMES = np.array(CONTROL_FILE_NAMES[:N])\n",
    "\n",
    "# we can create a padding function in order to pad \n",
    "def pad(locations, maxlen = 90, padding = \"post\", truncating = \"post\"): \n",
    "    new_locations = locations.tolist() \n",
    "    empty_row = np.zeros((1, 6))\n",
    "    for i, video in tqdm(enumerate(new_locations)): \n",
    "        if len(video) < maxlen:  \n",
    "            for new_row in range(maxlen - len(video)): \n",
    "                if padding == \"post\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([new_locations[i], empty_row])\n",
    "                if padding == \"pre\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([empty_row, new_locations[i]])\n",
    "\n",
    "        if len(video) > maxlen: \n",
    "            if truncating == \"post\": \n",
    "                new_locations[i] = new_locations[i][:maxlen]\n",
    "            elif truncating == \"pre\": \n",
    "                new_locations[i] = new_locations[i][len(video) - maxlen : ]\n",
    "    return np.array(new_locations)\n",
    "\n",
    "padded_armflapping_locations = ARMFLAPPING_LOCATIONS\n",
    "padded_control_locations = CONTROL_LOCATIONS\n",
    "padded_armflapping_locations = pad(padded_armflapping_locations, maxlen = 90)\n",
    "padded_control_locations = pad(padded_control_locations, maxlen = 90)\n",
    "print(padded_control_locations.shape, padded_armflapping_locations.shape)\n",
    "assert padded_armflapping_locations.shape == padded_control_locations.shape \n",
    "\n",
    "ARMFLAPPING_FILE_NAMES = np.array(ARMFLAPPING_FILE_NAMES)\n",
    "CONTROL_FILE_NAMES = np.array(CONTROL_FILE_NAMES)\n",
    "\n",
    "\"\"\"\n",
    "# first shuffle \n",
    "N = padded_armflapping_locations.shape[0]\n",
    "perm = np.random.permutation(N)\n",
    "padded_armflapping_locations = padded_armflapping_locations[perm]\n",
    "padded_control_locations = padded_control_locations[perm]\n",
    "ARMFLAPPING_FILE_NAMES = ARMFLAPPING_FILE_NAMES[perm]\n",
    "CONTROL_FILE_NAMES = CONTROL_FILE_NAMES[perm]\n",
    "\"\"\"\n",
    "\n",
    "# then balance each fold (still should do this) \n",
    "padded_armflapping_locations = np.array_split(padded_armflapping_locations, K)\n",
    "padded_control_locations = np.array_split(padded_control_locations, K)\n",
    "ARMFLAPPING_FILE_NAMES = np.array_split(ARMFLAPPING_FILE_NAMES, K)\n",
    "CONTROL_FILE_NAMES = np.array_split(CONTROL_FILE_NAMES, K)\n",
    "\n",
    "X_splits = []\n",
    "y_splits = []\n",
    "\n",
    "for i in range(K): \n",
    "    X_splits.append(np.concatenate([padded_armflapping_locations[i], padded_control_locations[i]]))\n",
    "    y_splits.append(np.concatenate([np.ones((len(padded_armflapping_locations[i]), 1)), np.zeros((len(padded_control_locations[i]), 1))]))\n",
    "\n",
    "import pickle\n",
    "\n",
    "for i, (X_split, y_split, A_FILE_NAMES, C_FILE_NAMES) in enumerate(zip(X_splits, y_splits, ARMFLAPPING_FILE_NAMES, CONTROL_FILE_NAMES)): \n",
    "    with open(f\"one_point_folds/split{i+1}\", 'wb') as f: \n",
    "        pickle.dump((X_split, y_split), f)\n",
    "    \n",
    "\n",
    "with open(f\"one_point_folds_seeds\", \"wb\") as f: \n",
    "    pickle.dump((padded_armflapping_locations, ARMFLAPPING_FILE_NAMES, padded_control_locations, CONTROL_FILE_NAMES), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-cf37e4d2f41e>:5: RuntimeWarning: Mean of empty slice.\n",
      "  return [locs[np.nonzero(locs)].mean() for locs in [X_locs_1, X_locs_2, Y_locs_1, Y_locs_2, Z_locs_1, Z_locs_2] ]\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "def mean_locs(frame): \n",
    "    \"\"\"take in a single frame, and return the mean location\"\"\"\n",
    "    assert len(frame) == 126, len(frame) # make sure this is a vector\n",
    "    X_locs_1, X_locs_2, Y_locs_1, Y_locs_2, Z_locs_1, Z_locs_2 = frame[:21], frame[21:42], frame[42:63], frame[63:84], frame[84:105], frame[105:]\n",
    "    return [locs[np.nonzero(locs)].mean() for locs in [X_locs_1, X_locs_2, Y_locs_1, Y_locs_2, Z_locs_1, Z_locs_2] ]\n",
    "\n",
    "new_X = [] \n",
    "for video in range(X.shape[0]): \n",
    "    matrix_video = X[video]\n",
    "    new_X.append(np.apply_along_axis(mean_locs, 1, matrix_video))\n",
    "    \n",
    "X = np.array(new_X, copy = True) \n",
    "assert len(X.shape) == 3\n",
    "\n",
    "splits = [] # stores k (X_i, y_i) splits\n",
    "X_splits, y_splits = np.array_split(X, K), np.array_split(y, K)\n",
    "for X_split, y_split in zip(X_splits, y_splits): \n",
    "    splits.append((X_split, y_split))\n",
    "\n",
    "for i, split in enumerate(splits): \n",
    "    with open(f\"mean_point_folds/split{i+1}\", 'wb') as f: \n",
    "        pickle.dump(split, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp_X.pkl\", 'wb') as f: \n",
    "    pickle.dump(X, f)\n",
    "\n",
    "with open(\"temp_X.pkl\", 'rb') as f: \n",
    "    X = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.nan_to_num(X)\n",
    "splits = [] # stores k (X_i, y_i) splits\n",
    "X_splits, y_splits = np.array_split(X, K), np.array_split(y, K)\n",
    "for X_split, y_split in zip(X_splits, y_splits): \n",
    "    splits.append((X_split, y_split))\n",
    "\n",
    "for i, split in enumerate(splits): \n",
    "    with open(f\"mean_point_folds/split{i+1}\", 'wb') as f: \n",
    "        pickle.dump(split, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"/Users/anish/Documents/Machine Learning Env/ActivRecognition-Autism-Diagnosis/behavior_data/control\"):\n",
    "         if os.path.isfile(\"/Users/anish/Documents/Machine Learning Env/ActivRecognition-Autism-Diagnosis/behavior_data/control/\" + file):\n",
    "             continue\n",
    "         else:\n",
    "             #shutil.rmtree(\"/Users/anish/Documents/Machine Learning Env/ActivRecognition-Autism-Diagnosis/behavior_data/armflapping/\" + file)\n",
    "             for vid_name in os.listdir(\"/Users/anish/Documents/Machine Learning Env/ActivRecognition-Autism-Diagnosis/behavior_data/control/\" + file):\n",
    "                 image = cv2.imread(\"/Users/anish/Documents/Machine Learning Env/ActivRecognition-Autism-Diagnosis/behavior_data/control/\" + file + \"/\" + vid_name)\n",
    "                 image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                 os.remove(\"/Users/anish/Documents/Machine Learning Env/ActivRecognition-Autism-Diagnosis/behavior_data/control/\" + file + \"/\" + vid_name)\n",
    "                 cv2.imwrite(\"/Users/anish/Documents/Machine Learning Env/ActivRecognition-Autism-Diagnosis/behavior_data/control/\" + file + \"/\" + vid_name, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
